"""
ultimate_lifecycle_api.py - API Ultimate pour la Gestion du Cycle de Vie des Mod√®les IA
Syst√®me complet avec toutes les fonctionnalit√©s avanc√©es

Lancez avec: uvicorn ai_lifecycle_api:app --host 0.0.0.0 --port 8006 --reload

Installation des d√©pendances:
pip install fastapi uvicorn pydantic numpy pandas scikit-learn reportlab fpdf pillow
"""

from fastapi import FastAPI, HTTPException, BackgroundTasks, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, StreamingResponse
from pydantic import BaseModel, Field, field_validator, model_validator
from typing import List, Optional, Dict, Any, Tuple
from datetime import datetime, timedelta
from enum import Enum
import numpy as np
import json
import math
import io
import base64
from collections import defaultdict
import hashlib
import uuid

# ============================================================
# CONFIGURATION
# ============================================================

app = FastAPI(
    title="Ultimate AI Lifecycle & Evolution Platform",
    description="Syst√®me complet et robuste de gestion du cycle de vie des mod√®les IA vers l'AGI",
    version="4.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Bases de donn√©es en m√©moire (remplacer par PostgreSQL/MongoDB en production)
MODELS_DB = {}
ANALYSES_DB = {}
CERTIFICATIONS_DB = {}
BENCHMARKS_DB = {}
MARKETPLACE_DB = {}
MENTORING_DB = {}
EVOLUTION_HISTORY = {}

# ============================================================
# ENUMS & CONSTANTES
# ============================================================

class AIAge(str, Enum):
    AGE_1 = "Age 1: Narrow AI"
    AGE_2 = "Age 2: Specialized AI"
    AGE_3 = "Age 3: Multi-Task AI"
    AGE_4 = "Age 4: Adaptive AI"
    AGE_5 = "Age 5: General AI"
    AGE_6 = "Age 6: Advanced General AI"
    AGE_7 = "Age 7: Super AI"
    AGI = "AGI: Artificial General Intelligence"

class CertificationLevel(str, Enum):
    BRONZE = "Bronze"
    SILVER = "Silver"
    GOLD = "Gold"
    PLATINUM = "Platinum"
    DIAMOND = "Diamond"

class Priority(str, Enum):
    CRITICAL = "CRITICAL"
    HIGH = "HIGH"
    MEDIUM = "MEDIUM"
    LOW = "LOW"

# Crit√®res d√©taill√©s pour chaque Age
AGE_CRITERIA = {
    AIAge.AGE_1: {
        "min_score": 0, "max_score": 30,
        "min_parameters": 1_000_000,
        "min_tokens": 512,
        "min_modalities": 1,
        "min_tasks": 1,
        "required_scores": {
            "reasoning": 40, "adaptability": 20, "generalization": 30,
            "creativity": 20, "learning_efficiency": 30, "transfer_learning": 10
        },
        "required_capabilities": ["basic_inference"],
        "certification_requirements": {
            "min_accuracy": 70,
            "min_benchmarks": 3,
            "documentation_required": True
        }
    },
    AIAge.AGE_2: {
        "min_score": 30, "max_score": 45,
        "min_parameters": 10_000_000,
        "min_tokens": 2048,
        "min_modalities": 1,
        "min_tasks": 3,
        "required_scores": {
            "reasoning": 55, "adaptability": 35, "generalization": 45,
            "creativity": 30, "learning_efficiency": 40, "transfer_learning": 25
        },
        "required_capabilities": ["basic_inference", "domain_expertise", "fine_tuning"],
        "certification_requirements": {
            "min_accuracy": 75,
            "min_benchmarks": 5,
            "documentation_required": True,
            "peer_review": True
        }
    },
    AIAge.AGE_3: {
        "min_score": 45, "max_score": 60,
        "min_parameters": 100_000_000,
        "min_tokens": 4096,
        "min_modalities": 2,
        "min_tasks": 5,
        "required_scores": {
            "reasoning": 65, "adaptability": 50, "generalization": 60,
            "creativity": 45, "learning_efficiency": 55, "transfer_learning": 40
        },
        "required_capabilities": ["multi_task", "context_understanding", "basic_reasoning"],
        "certification_requirements": {
            "min_accuracy": 80,
            "min_benchmarks": 8,
            "documentation_required": True,
            "peer_review": True,
            "safety_audit": True
        }
    },
    AIAge.AGE_4: {
        "min_score": 60, "max_score": 72,
        "min_parameters": 1_000_000_000,
        "min_tokens": 8192,
        "min_modalities": 2,
        "min_tasks": 10,
        "required_scores": {
            "reasoning": 75, "adaptability": 65, "generalization": 70,
            "creativity": 60, "learning_efficiency": 70, "transfer_learning": 60
        },
        "required_capabilities": ["adaptive_learning", "few_shot_learning", "meta_learning"],
        "certification_requirements": {
            "min_accuracy": 82,
            "min_benchmarks": 12,
            "documentation_required": True,
            "peer_review": True,
            "safety_audit": True,
            "ethics_review": True
        }
    },
    AIAge.AGE_5: {
        "min_score": 72, "max_score": 82,
        "min_parameters": 10_000_000_000,
        "min_tokens": 16384,
        "min_modalities": 3,
        "min_tasks": 20,
        "required_scores": {
            "reasoning": 82, "adaptability": 75, "generalization": 80,
            "creativity": 70, "learning_efficiency": 80, "transfer_learning": 75
        },
        "required_capabilities": ["general_reasoning", "cross_domain", "zero_shot_learning", "planning"],
        "certification_requirements": {
            "min_accuracy": 85,
            "min_benchmarks": 15,
            "documentation_required": True,
            "peer_review": True,
            "safety_audit": True,
            "ethics_review": True,
            "regulatory_compliance": True
        }
    },
    AIAge.AGE_6: {
        "min_score": 82, "max_score": 90,
        "min_parameters": 100_000_000_000,
        "min_tokens": 32768,
        "min_modalities": 4,
        "min_tasks": 50,
        "required_scores": {
            "reasoning": 90, "adaptability": 85, "generalization": 88,
            "creativity": 82, "learning_efficiency": 88, "transfer_learning": 85
        },
        "required_capabilities": ["advanced_reasoning", "abstract_thinking", "causal_inference", "self_improvement"],
        "certification_requirements": {
            "min_accuracy": 90,
            "min_benchmarks": 20,
            "documentation_required": True,
            "peer_review": True,
            "safety_audit": True,
            "ethics_review": True,
            "regulatory_compliance": True,
            "continuous_monitoring": True
        }
    },
    AIAge.AGE_7: {
        "min_score": 90, "max_score": 95,
        "min_parameters": 1_000_000_000_000,
        "min_tokens": 65536,
        "min_modalities": 5,
        "min_tasks": 100,
        "required_scores": {
            "reasoning": 95, "adaptability": 92, "generalization": 94,
            "creativity": 90, "learning_efficiency": 95, "transfer_learning": 93
        },
        "required_capabilities": ["superintelligence", "recursive_self_improvement", "universal_learning"],
        "certification_requirements": {
            "min_accuracy": 95,
            "min_benchmarks": 30,
            "documentation_required": True,
            "peer_review": True,
            "safety_audit": True,
            "ethics_review": True,
            "regulatory_compliance": True,
            "continuous_monitoring": True,
            "international_oversight": True
        }
    },
    AIAge.AGI: {
        "min_score": 95, "max_score": 100,
        "min_parameters": 10_000_000_000_000,
        "min_tokens": 131072,
        "min_modalities": 10,
        "min_tasks": 1000,
        "required_scores": {
            "reasoning": 98, "adaptability": 97, "generalization": 98,
            "creativity": 95, "learning_efficiency": 98, "transfer_learning": 97
        },
        "required_capabilities": ["full_agi", "consciousness", "general_problem_solving", "human_level_intelligence"],
        "certification_requirements": {
            "min_accuracy": 98,
            "min_benchmarks": 50,
            "documentation_required": True,
            "peer_review": True,
            "safety_audit": True,
            "ethics_review": True,
            "regulatory_compliance": True,
            "continuous_monitoring": True,
            "international_oversight": True,
            "philosophical_review": True
        }
    }
}
AGE_THRESHOLDS = {
    AIAge.AGE_1: {"min": 0, "max": 30},
    AIAge.AGE_2: {"min": 30, "max": 45},
    AIAge.AGE_3: {"min": 45, "max": 60},
    AIAge.AGE_4: {"min": 60, "max": 72},
    AIAge.AGE_5: {"min": 72, "max": 82},
    AIAge.AGE_6: {"min": 82, "max": 90},
    AIAge.AGE_7: {"min": 90, "max": 95},
    AIAge.AGI: {"min": 95, "max": 100}
}
# ============================================================
# MOD√àLES PYDANTIC
# ============================================================

from pydantic import BaseModel, Field, field_validator
from typing import Optional, List, Dict

class ModelCharacteristics(BaseModel):
    name: str = Field(..., min_length=1, max_length=100)
    version: str = Field(default="1.0")
    description: Optional[str] = Field(None, max_length=1000)
    
    # Architecture
    architecture: str
    parameters: int = Field(gt=0, description="Nombre total de param√®tres")
    layers: int = Field(gt=0)
    hidden_size: int = Field(gt=0)
    attention_heads: Optional[int] = Field(None, ge=0)
    
    # Capacit√©s
    max_tokens: int = Field(gt=0)
    context_window: int = Field(gt=0)
    modalities: List[str] = Field(min_items=1)
    supported_tasks: List[str] = Field(min_items=1)
    
    # Technologies
    framework: str
    training_data_size: Optional[float] = Field(None, ge=0, description="En TB")
    training_duration: Optional[float] = Field(None, ge=0, description="En heures")
    hardware: Optional[str] = None
    
    # Performance
    inference_speed: Optional[float] = Field(None, ge=0, description="tokens/sec")
    memory_usage: Optional[float] = Field(None, ge=0, description="GB")
    energy_efficiency: Optional[float] = Field(None, ge=0, description="FLOPS/W")
    
    # Capacit√©s avanc√©es
    capabilities: List[str] = Field(default_factory=list)
    fine_tuning_capable: bool = False
    few_shot_learning: bool = False
    zero_shot_learning: bool = False
    transfer_learning: bool = False
    meta_learning: bool = False
    
    # S√©curit√© et √©thique
    safety_measures: Optional[List[str]] = Field(default_factory=list)
    ethical_guidelines: Optional[List[str]] = Field(default_factory=list)
    bias_mitigation: bool = False
    explainability_features: bool = False
    
    # Benchmarks
    benchmark_scores: Dict[str, float] = Field(default_factory=dict)

    @field_validator("parameters", mode="before")
    @classmethod
    def parse_and_validate_parameters(cls, v):
        if isinstance(v, str):
            val = v.replace(",", "").strip().upper()
            if val.endswith("B"):  # milliards
                v = float(val[:-1]) * 1e9
            elif val.endswith("M"):  # millions
                v = float(val[:-1]) * 1e6
            elif val.endswith("K"):  # milliers
                v = float(val[:-1]) * 1e3
            else:
                v = float(val)
        v = int(v)
        if v < 1000:
            raise ValueError("Le mod√®le doit avoir au moins 1000 param√®tres")
        return v

class EvaluationScores(BaseModel):
    reasoning: float = Field(ge=0, le=100)
    adaptability: float = Field(ge=0, le=100)
    generalization: float = Field(ge=0, le=100)
    creativity: float = Field(ge=0, le=100)
    learning_efficiency: float = Field(ge=0, le=100)
    transfer_learning: float = Field(ge=0, le=100)
    robustness: float = Field(ge=0, le=100)
    interpretability: float = Field(ge=0, le=100)
    safety: float = Field(ge=0, le=100)
    ethical_alignment: float = Field(ge=0, le=100)

    @model_validator(mode="after")
    def validate_scores(self):
        for field, value in self.__dict__.items():
            if not 0 <= value <= 100:
                raise ValueError(f"Le champ '{field}' doit √™tre entre 0 et 100")
            setattr(self, field, round(value, 2))
        return self

class AnalysisRequest(BaseModel):
    characteristics: ModelCharacteristics
    scores: EvaluationScores
    request_certification: bool = False
    enable_mentoring: bool = False

class BenchmarkSubmission(BaseModel):
    model_id: str
    benchmark_name: str
    score: float = Field(ge=0, le=100)
    details: Optional[Dict[str, Any]] = None

class MentoringRequest(BaseModel):
    mentee_model_id: str
    target_age: AIAge
    areas_of_improvement: List[str]

# ============================================================
# MOTEUR D'ANALYSE AVANC√â
# ============================================================

class UltimateLifecycleEngine:
    """Moteur d'analyse ultra-sophistiqu√©"""
    
    @staticmethod
    def calculate_maturity_score(chars: ModelCharacteristics, scores: EvaluationScores) -> float:
        """Calcul du score de maturit√© avec pond√©ration dynamique"""
        
        # Scores normalis√©s
        param_score = min(100, (math.log10(chars.parameters) / 13) * 100)
        token_score = min(100, (math.log2(chars.max_tokens) / 17) * 100)
        modality_score = min(100, len(chars.modalities) * 12.5)
        task_score = min(100, len(chars.supported_tasks) * 3.33)
        capability_score = min(100, len(chars.capabilities) * 2.5)
        
        # Score d'√©valuation pond√©r√©
        eval_weights = {
            'reasoning': 1.5, 'adaptability': 1.3, 'generalization': 1.4,
            'creativity': 1.1, 'learning_efficiency': 1.3, 'transfer_learning': 1.4,
            'robustness': 1.2, 'interpretability': 1.0, 'safety': 1.6, 'ethical_alignment': 1.5
        }
        
        weighted_eval_sum = sum(getattr(scores, k) * v for k, v in eval_weights.items())
        weighted_eval_avg = weighted_eval_sum / sum(eval_weights.values())
        
        # Bonus pour capacit√©s avanc√©es
        bonus = 0
        if chars.meta_learning:
            bonus += 5
        if chars.zero_shot_learning:
            bonus += 3
        if chars.few_shot_learning:
            bonus += 2
        if chars.bias_mitigation:
            bonus += 2
        if chars.explainability_features:
            bonus += 2
        
        # Score final avec pond√©ration optimis√©e
        maturity = (
            param_score * 0.15 +
            token_score * 0.08 +
            modality_score * 0.10 +
            task_score * 0.12 +
            capability_score * 0.15 +
            weighted_eval_avg * 0.40 +
            bonus
        )
        
        return round(min(100, maturity), 2)
    
    @staticmethod
    def determine_age_with_confidence(maturity_score: float, chars: ModelCharacteristics, 
                                     scores: EvaluationScores) -> Tuple[AIAge, float, Dict]:
        """D√©termine l'Age avec analyse de confiance d√©taill√©e"""
        
        age_scores = {}
        
        for age, criteria in AGE_CRITERIA.items():
            score = 0
            max_score = 0
            details = {}
            
            # V√©rification du score de maturit√©
            if criteria["min_score"] <= maturity_score < criteria["max_score"]:
                score += 10
            max_score += 10
            
            # V√©rification des param√®tres
            if chars.parameters >= criteria["min_parameters"]:
                score += 8
                details["parameters"] = "‚úì"
            else:
                details["parameters"] = f"‚úó ({chars.parameters:,} / {criteria['min_parameters']:,})"
            max_score += 8
            
            # V√©rification des tokens
            if chars.max_tokens >= criteria["min_tokens"]:
                score += 6
                details["tokens"] = "‚úì"
            else:
                details["tokens"] = f"‚úó ({chars.max_tokens} / {criteria['min_tokens']})"
            max_score += 6
            
            # V√©rification des modalit√©s
            if len(chars.modalities) >= criteria["min_modalities"]:
                score += 5
                details["modalities"] = "‚úì"
            else:
                details["modalities"] = f"‚úó ({len(chars.modalities)} / {criteria['min_modalities']})"
            max_score += 5
            
            # V√©rification des t√¢ches
            if len(chars.supported_tasks) >= criteria["min_tasks"]:
                score += 5
                details["tasks"] = "‚úì"
            else:
                details["tasks"] = f"‚úó ({len(chars.supported_tasks)} / {criteria['min_tasks']})"
            max_score += 5
            
            # V√©rification des scores requis
            score_checks = 0
            for score_name, required in criteria["required_scores"].items():
                if getattr(scores, score_name) >= required:
                    score_checks += 1
            
            score += (score_checks / len(criteria["required_scores"])) * 15
            max_score += 15
            details["scores"] = f"{score_checks}/{len(criteria['required_scores'])}"
            
            # V√©rification des capacit√©s
            has_capabilities = sum(1 for cap in criteria["required_capabilities"] 
                                 if cap in chars.capabilities)
            score += (has_capabilities / len(criteria["required_capabilities"])) * 10
            max_score += 10
            details["capabilities"] = f"{has_capabilities}/{len(criteria['required_capabilities'])}"
            
            age_scores[age] = {
                "score": score,
                "max_score": max_score,
                "percentage": round((score / max_score) * 100, 1),
                "details": details
            }
        
        # Trouver le meilleur match
        best_age = max(age_scores.items(), key=lambda x: x[1]["percentage"])
        
        return best_age[0], best_age[1]["percentage"], age_scores
    
    @staticmethod
    def calculate_comprehensive_gaps(current_age: AIAge, next_age: Optional[AIAge],
                                    chars: ModelCharacteristics, scores: EvaluationScores,
                                    maturity_score: float) -> List[Dict]:
        """Calcul d√©taill√© des √©carts avec priorisation"""
        
        if not next_age:
            return []
        
        gaps = []
        next_criteria = AGE_CRITERIA[next_age]
        
        # Gap de maturit√©
        maturity_gap = next_criteria["min_score"] - maturity_score
        if maturity_gap > 0:
            gaps.append({
                "metric": "Score de Maturit√© Global",
                "category": "Overall",
                "current": maturity_score,
                "required": next_criteria["min_score"],
                "gap": round(maturity_gap, 1),
                "percentage": round((maturity_score / next_criteria["min_score"]) * 100, 1),
                "priority": "CRITICAL" if maturity_gap > 20 else "HIGH",
                "estimated_effort_months": round(maturity_gap / 2, 1),
                "impact_score": 10
            })
        
        # Gaps architecturaux
        if chars.parameters < next_criteria["min_parameters"]:
            gap_val = next_criteria["min_parameters"] - chars.parameters
            gaps.append({
                "metric": "Param√®tres du Mod√®le",
                "category": "Architecture",
                "current": chars.parameters,
                "required": next_criteria["min_parameters"],
                "gap": gap_val,
                "percentage": round((chars.parameters / next_criteria["min_parameters"]) * 100, 1),
                "priority": "HIGH",
                "estimated_effort_months": 6,
                "impact_score": 9
            })
        
        if chars.max_tokens < next_criteria["min_tokens"]:
            gaps.append({
                "metric": "Capacit√© de Tokens",
                "category": "Architecture",
                "current": chars.max_tokens,
                "required": next_criteria["min_tokens"],
                "gap": next_criteria["min_tokens"] - chars.max_tokens,
                "percentage": round((chars.max_tokens / next_criteria["min_tokens"]) * 100, 1),
                "priority": "MEDIUM",
                "estimated_effort_months": 3,
                "impact_score": 7
            })
        
        # Gaps de modalit√©s
        if len(chars.modalities) < next_criteria["min_modalities"]:
            gaps.append({
                "metric": "Modalit√©s Support√©es",
                "category": "Capabilities",
                "current": len(chars.modalities),
                "required": next_criteria["min_modalities"],
                "gap": next_criteria["min_modalities"] - len(chars.modalities),
                "percentage": round((len(chars.modalities) / next_criteria["min_modalities"]) * 100, 1),
                "priority": "HIGH",
                "estimated_effort_months": 4,
                "impact_score": 8
            })
        
        # Gaps de t√¢ches
        if len(chars.supported_tasks) < next_criteria["min_tasks"]:
            gaps.append({
                "metric": "T√¢ches Support√©es",
                "category": "Capabilities",
                "current": len(chars.supported_tasks),
                "required": next_criteria["min_tasks"],
                "gap": next_criteria["min_tasks"] - len(chars.supported_tasks),
                "percentage": round((len(chars.supported_tasks) / next_criteria["min_tasks"]) * 100, 1),
                "priority": "MEDIUM",
                "estimated_effort_months": 5,
                "impact_score": 7
            })
        
        # Gaps de scores
        for score_name, required in next_criteria["required_scores"].items():
            current_score = getattr(scores, score_name)
            if current_score < required:
                gaps.append({
                    "metric": score_name.replace('_', ' ').title(),
                    "category": "Performance",
                    "current": current_score,
                    "required": required,
                    "gap": round(required - current_score, 1),
                    "percentage": round((current_score / required) * 100, 1),
                    "priority": "HIGH" if required - current_score > 15 else "MEDIUM",
                    "estimated_effort_months": round((required - current_score) / 5, 1),
                    "impact_score": 8
                })
        
        # Gaps de capacit√©s
        missing_caps = set(next_criteria["required_capabilities"]) - set(chars.capabilities)
        if missing_caps:
            gaps.append({
                "metric": "Capacit√©s Requises",
                "category": "Capabilities",
                "current": len(chars.capabilities),
                "required": len(next_criteria["required_capabilities"]),
                "gap": len(missing_caps),
                "missing_capabilities": list(missing_caps),
                "percentage": round((len(chars.capabilities) / len(next_criteria["required_capabilities"])) * 100, 1),
                "priority": "CRITICAL",
                "estimated_effort_months": len(missing_caps) * 2,
                "impact_score": 10
            })
        
        # Trier par priorit√© et impact
        priority_order = {"CRITICAL": 4, "HIGH": 3, "MEDIUM": 2, "LOW": 1}
        gaps.sort(key=lambda x: (priority_order[x["priority"]], x["impact_score"]), reverse=True)
        
        return gaps
    
    @staticmethod
    def generate_advanced_recommendations(current_age: AIAge, gaps: List[Dict],
                                         scores: EvaluationScores, chars: ModelCharacteristics) -> List[Dict]:
        """G√©n√®re des recommandations ultra-d√©taill√©es"""
        
        recommendations = []
        
        # Recommandations bas√©es sur les gaps critiques
        critical_gaps = [g for g in gaps if g.get("priority") == "CRITICAL"]
        
        for gap in critical_gaps:
            if "Maturit√©" in gap["metric"]:
                recommendations.append({
                    "id": str(uuid.uuid4()),
                    "title": "Am√©lioration Globale du Mod√®le",
                    "category": "Strategic",
                    "priority": "CRITICAL",
                    "description": f"Augmenter le score de maturit√© de {gap['gap']:.1f} points",
                    "action_steps": [
                        "Augmenter la taille du mod√®le",
                        "Am√©liorer la diversit√© des donn√©es d'entra√Ænement",
                        "Optimiser l'architecture",
                        "Impl√©menter des techniques avanc√©es d'apprentissage"
                    ],
                    "estimated_time": f"{gap.get('estimated_effort_months', 6)} mois",
                    "estimated_cost": "$$$$",
                    "impact": "Tr√®s √©lev√©",
                    "difficulty": "√âlev√©",
                    "prerequisites": [],
                    "expected_improvement": f"+{gap['gap']:.1f} points de maturit√©",
                    "resources_needed": [
                        "√âquipe ML exp√©riment√©e",
                        "Infrastructure de calcul avanc√©e",
                        "Datasets de qualit√©"
                    ]
                })
            
            elif "Param√®tres" in gap["metric"]:
                recommendations.append({
                    "id": str(uuid.uuid4()),
                    "title": "Scale-up du Mod√®le",
                    "category": "Architecture",
                    "priority": "HIGH",
                    "description": f"Augmenter √† {gap['required']:,} param√®tres",
                    "action_steps": [
                        "Analyser les goulots d'√©tranglement actuels",
                        "Concevoir une architecture scalable",
                        "Planifier les ressources de calcul",
                        "Impl√©menter progressivement"
                    ],
                    "estimated_time": "6-12 mois",
                    "estimated_cost": "$$$$$",
                    "impact": "Tr√®s √©lev√©",
                    "difficulty": "Tr√®s √©lev√©",
                    "prerequisites": ["Infrastructure GPU/TPU", "Budget cons√©quent"],
                    "expected_improvement": "Capacit√©s accrues significatives",
                    "resources_needed": [
                        "Cluster GPU/TPU haute performance",
                        "√âquipe DevOps",
                        "Budget infrastructure > $1M"
                    ]
                })
        
        # Recommandations bas√©es sur les scores faibles
        low_scores = [(k, v) for k, v in scores.dict().items() if v < 70]
        
        for score_name, score_value in low_scores:
            if score_name == "reasoning":
                recommendations.append({
                    "id": str(uuid.uuid4()),
                    "title": "Am√©lioration du Raisonnement",
                    "category": "Performance",
                    "priority": "HIGH",
                    "description": f"Augmenter le score de raisonnement de {70 - score_value:.1f} points",
                    "action_steps": [
                        "Impl√©menter Chain-of-Thought prompting",
                        "Ajouter des m√©canismes de reasoning explicite",
                        "Entra√Æner sur des datasets de raisonnement",
                        "Int√©grer des knowledge graphs"
                    ],
                    "estimated_time": "4-6 mois",
                    "estimated_cost": "$$$",
                    "impact": "√âlev√©",
                    "difficulty": "Moyen",
                    "prerequisites": ["Expertise en NLP avanc√©"],
                    "expected_improvement": f"+{70 - score_value:.1f} points",
                    "resources_needed": [
                        "Datasets de raisonnement",
                        "Experts en reasoning",
                        "GPU pour fine-tuning"
                    ]
                })
            
            elif score_name == "safety":
                recommendations.append({
                    "id": str(uuid.uuid4()),
                    "title": "Renforcement de la S√©curit√©",
                    "category": "Safety",
                    "priority": "CRITICAL",
                    "description": "Am√©liorer les m√©canismes de s√©curit√© et d'alignement",
                    "action_steps": [
                        "Impl√©menter RLHF (Reinforcement Learning from Human Feedback)",
                        "Ajouter des guardrails de s√©curit√©",
                        "Effectuer des red-teaming tests",
                        "Mettre en place un monitoring continu"
                    ],
                    "estimated_time": "3-5 mois",
                    "estimated_cost": "$$$$",
                    "impact": "Critique",
                    "difficulty": "√âlev√©",
                    "prerequisites": ["√âquipe s√©curit√© IA"],
                    "expected_improvement": "R√©duction significative des risques",
                    "resources_needed": [
                        "Experts en AI safety",
                        "Labelers humains",
                        "Infrastructure de monitoring"
                    ]
                })
def calculate_maturity(chars: ModelCharacteristics, scores: EvaluationScores) -> float:
        """Calcule le score de maturit√©"""
        param_score = min(100, (math.log10(chars.parameters) / 13) * 100)
        token_score = min(100, (math.log2(chars.max_tokens) / 17) * 100)
        modality_score = min(100, len(chars.modalities) * 12.5)
        task_score = min(100, len(chars.supported_tasks) * 3.33)
        capability_score = min(100, len(chars.capabilities) * 2.5)
        
        eval_avg = sum([
            scores.reasoning, scores.adaptability, scores.generalization,
            scores.creativity, scores.learning_efficiency, scores.transfer_learning,
            scores.robustness, scores.interpretability, scores.safety, scores.ethical_alignment
        ]) / 10
    
        bonus = 0
        if chars.meta_learning: bonus += 5
        if chars.zero_shot_learning: bonus += 3
        if chars.bias_mitigation: bonus += 2
        
        maturity = (
            param_score * 0.15 +
            token_score * 0.08 +
            modality_score * 0.10 +
            task_score * 0.12 +
            capability_score * 0.15 +
            eval_avg * 0.40 +
            bonus
        )
        
        return round(min(100, maturity), 2)

def determine_age(maturity: float) -> tuple:
        """D√©termine l'Age"""
        for age, thresholds in AGE_THRESHOLDS.items():
            if thresholds["min"] <= maturity < thresholds["max"]:
                range_size = thresholds["max"] - thresholds["min"]
                position = maturity - thresholds["min"]
                confidence = 70 + (position / range_size) * 20
                return age, round(confidence, 1)
        return AIAge.AGI, 95.0

def calculate_gaps(current_age: AIAge, next_age: Optional[AIAge], maturity: float) -> List[Dict]:
    """Calcule les gaps"""
    if not next_age:
        return []
    
    gaps = []
    next_threshold = AGE_THRESHOLDS[next_age]["min"]
    
    gap_value = next_threshold - maturity
    if gap_value > 0:
        gaps.append({
            "metric": "Score de Maturit√©",
            "category": "Overall",
            "current": maturity,
            "required": next_threshold,
            "gap": round(gap_value, 1),
            "percentage": round((maturity / next_threshold) * 100, 1),
            "priority": "CRITICAL" if gap_value > 20 else "HIGH",
            "estimated_effort_months": round(gap_value / 2, 1),
            "impact_score": 10
        })
    
    return gaps

def generate_recommendations(gaps: List[Dict]) -> List[Dict]:
    """G√©n√®re des recommandations"""
    recommendations = []
    
    for i, gap in enumerate(gaps[:5], 1):
        recommendations.append({
            "id": str(uuid.uuid4()),
            "title": f"Am√©liorer {gap['metric']}",
            "category": gap['category'],
            "priority": gap['priority'],
            "description": f"Augmenter de {gap['gap']:.1f} points",
            "action_steps": [
                "Analyser les points faibles",
                "Mettre en place un plan d'action",
                "Impl√©menter les am√©liorations",
                "Valider les r√©sultats"
            ],
            "estimated_time": f"{gap['estimated_effort_months']} mois",
            "estimated_cost": "$$" * min(5, int(gap['estimated_effort_months'] / 2)),
            "impact": "√âlev√©" if gap['priority'] in ['CRITICAL', 'HIGH'] else "Moyen",
            "difficulty": "√âlev√©",
            "prerequisites": [],
            "expected_improvement": f"+{gap['gap']:.1f} points"
        })
    
    return recommendations

def predict_timeline(gaps: List[Dict]) -> Dict:
    """Pr√©dit la timeline"""
    if not gaps:
        return {"total_months": 0, "phases": [], "confidence": 100, "milestones": []}
    
    total_months = sum(g.get("estimated_effort_months", 6) for g in gaps)
    
    milestones = []
    for i in range(5):
        progress = i * 25
        month = int(total_months * i / 4)
        date = (datetime.now() + timedelta(days=30 * month)).strftime("%Y-%m-%d")
        
        milestones.append({
            "milestone_id": i + 1,
            "name": ["D√©but", "Premier Jalon", "Mi-parcours", "Sprint Final", "Objectif"][i],
            "month": month,
            "date": date,
            "progress_percentage": progress,
            "key_deliverables": [f"Livrable {i+1}"],
            "success_criteria": [f"Crit√®re {i+1}"]
        })
    
    return {
        "total_months": round(total_months, 1),
        "range_min": round(total_months * 0.7, 1),
        "range_max": round(total_months * 1.5, 1),
        "phases": [],
        "confidence": round(max(50, 90 - len(gaps) * 5), 1),
        "milestones": milestones
    }



# ============================================================
# ROUTES API
# ============================================================

@app.get("/")
async def root():
    return {
        "message": "AI Lifecycle Platform v4.0",
        "status": "operational",
        "documentation": "/docs"
    }

@app.get("/health")
async def health():
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "models_count": len(MODELS_DB)
    }

@app.post("/api/v1/analyze")
async def analyze_model(request: AnalysisRequest):
    """Analyse compl√®te d'un mod√®le"""
    try:
        chars = request.characteristics
        scores = request.scores
        
        # Calculs
        maturity = calculate_maturity(chars, scores)
        current_age, confidence = determine_age(maturity)
        
        ages_list = list(AIAge)
        current_idx = ages_list.index(current_age)
        next_age = ages_list[current_idx + 1] if current_idx < len(ages_list) - 1 else None
        
        gaps = calculate_gaps(current_age, next_age, maturity)
        recommendations = generate_recommendations(gaps)
        timeline = predict_timeline(gaps)
        
        success_prob = round(
            max(20, min(95, sum(g["percentage"] for g in gaps) / len(gaps) - len(gaps) * 2))
            if gaps else 95, 1
        )
        
        model_id = f"{chars.name.replace(' ', '_')}_{hashlib.md5(str(datetime.now()).encode()).hexdigest()[:8]}"
        
        analysis = {
            "model_id": model_id,
            "timestamp": datetime.now().isoformat(),
            "model_info": {
                "characteristics": chars.dict(),
                "scores": scores.dict()
            },
            "maturity_analysis": {
                "overall_score": maturity,
                "current_age": current_age.value,
                "next_age": next_age.value if next_age else "AGI Atteint",
                "confidence": confidence,
                "progress_to_next_age": round((maturity - AGE_THRESHOLDS[current_age]["min"]) / 
                                             (AGE_THRESHOLDS[current_age]["max"] - AGE_THRESHOLDS[current_age]["min"]) * 100, 1)
                                       if next_age else 100
            },
            "gap_analysis": {
                "total_gaps": len(gaps),
                "critical_gaps": len([g for g in gaps if g["priority"] == "CRITICAL"]),
                "gaps": gaps,
                "summary": {
                    "architecture": len([g for g in gaps if g["category"] == "Architecture"]),
                    "performance": len([g for g in gaps if g["category"] == "Performance"]),
                    "capabilities": len([g for g in gaps if g["category"] == "Capabilities"])
                }
            },
            "recommendations": {
                "total": len(recommendations),
                "items": recommendations
            },
            "evolution_prediction": {
                "timeline": timeline,
                "success_probability": success_prob
            },
            "ai_insights": {
                "strengths": [f"{k.replace('_', ' ').title()}: {v:.1f}%" 
                            for k, v in scores.dict().items() if v >= 80][:5],
                "weaknesses": [f"{k.replace('_', ' ').title()}: {v:.1f}%" 
                             for k, v in scores.dict().items() if v < 60][:5],
                "key_focus_areas": [g["metric"] for g in gaps[:3]],
                "quick_wins": [g["metric"] for g in gaps if g.get("estimated_effort_months", 12) <= 3][:3]
            }
        }
        
        MODELS_DB[model_id] = analysis
        ANALYSES_DB[model_id] = {
            "name": chars.name,
            "version": chars.version,
            "age": current_age.value,
            "maturity": maturity,
            "timestamp": datetime.now().isoformat()
        }
        
        if model_id not in EVOLUTION_HISTORY:
            EVOLUTION_HISTORY[model_id] = []
        EVOLUTION_HISTORY[model_id].append({
            "timestamp": datetime.now().isoformat(),
            "maturity_score": maturity,
            "age": current_age.value
        })
        
        return {"success": True, "model_id": model_id, "analysis": analysis}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/v1/models")
async def list_models():
    """Liste tous les mod√®les"""
    return {
        "total": len(ANALYSES_DB),
        "models": sorted(list(ANALYSES_DB.values()), key=lambda x: x["maturity"], reverse=True)
    }

@app.get("/api/v1/models/{model_id}")
async def get_model(model_id: str):
    """R√©cup√®re un mod√®le"""
    if model_id not in MODELS_DB:
        raise HTTPException(status_code=404, detail="Mod√®le non trouv√©")
    return {"success": True, "analysis": MODELS_DB[model_id]}

@app.get("/api/v1/stats")
async def get_stats():
    """Statistiques globales"""
    if not ANALYSES_DB:
        return {
            "total_models": 0,
            "average_maturity": 0,
            "age_distribution": {}
        }
    
    models = list(ANALYSES_DB.values())
    maturities = [m["maturity"] for m in models]
    
    age_dist = {}
    for model in models:
        age = model["age"]
        age_dist[age] = age_dist.get(age, 0) + 1
    
    return {
        "total_models": len(models),
        "average_maturity": round(sum(maturities) / len(maturities), 2),
        "median_maturity": round(sorted(maturities)[len(maturities) // 2], 2),
        "min_maturity": round(min(maturities), 2),
        "max_maturity": round(max(maturities), 2),
        "age_distribution": age_dist
    }

@app.get("/api/v1/leaderboard")
async def get_leaderboard(limit: int = 50):
    """Leaderboard"""
    models = list(ANALYSES_DB.values())
    sorted_models = sorted(models, key=lambda x: x["maturity"], reverse=True)[:limit]
    
    return {
        "total": len(sorted_models),
        "leaderboard": [{"rank": i + 1, **model} for i, model in enumerate(sorted_models)]
    }

@app.get("/api/v1/marketplace/datasets")
async def get_marketplace():
    """Marketplace de datasets"""
    datasets = [
        {
            "id": "ds_001",
            "name": "Advanced Reasoning Dataset",
            "description": "10K exemples de raisonnement complexe",
            "category": "Reasoning",
            "size": "2.5 GB",
            "price": "$299",
            "rating": 4.8,
            "downloads": 1234,
            "improvement_potential": {"reasoning": "+15-20%"},
            "compatible_ages": ["Age 3", "Age 4", "Age 5"]
        }
    ]
    
    return {"total": len(datasets), "datasets": datasets, "categories": ["Reasoning"]}

@app.delete("/api/v1/models/{model_id}")
async def delete_model(model_id: str):
    """Supprime un mod√®le"""
    if model_id not in MODELS_DB:
        raise HTTPException(status_code=404, detail="Mod√®le non trouv√©")
    
    del MODELS_DB[model_id]
    del ANALYSES_DB[model_id]
    
    return {"success": True, "message": "Mod√®le supprim√©"}

if __name__ == "__main__":
    import uvicorn
    print("="*70)
    print("üöÄ AI LIFECYCLE PLATFORM API v4.0")
    print("="*70)
    print("\n‚úÖ API d√©marr√©e avec succ√®s!")
    print("\nüåê Acc√®s:")
    print("  üìñ Documentation: http://localhost:8000/docs")
    print("  üîç Health Check: http://localhost:8000/health")
    print("\n" + "="*70)
    
    uvicorn.run(app, host="0.0.0.0", port=8006)