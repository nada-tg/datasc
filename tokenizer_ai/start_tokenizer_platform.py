# start_tokenizer_platform.py - Script de dÃ©marrage pour la plateforme de tokenizer universel

import subprocess
import sys
import time
import os
from pathlib import Path

def check_tokenizer_dependencies():
    """VÃ©rifie les dÃ©pendances spÃ©cifiques au tokenizer"""
    required_packages = [
        'fastapi', 'uvicorn', 'streamlit', 'transformers', 'tokenizers', 
        'sentencepiece', 'spacy', 'polyglot', 'langdetect', 'textstat',
        'nltk', 'scikit-learn', 'fasttext', 'pandas', 'numpy', 'plotly',
        'requests'
    ]
    
    missing_packages = []
    for package in required_packages:
        try:
            if package == 'polyglot':
                # Polyglot peut Ãªtre problÃ©matique, le rendre optionnel
                continue
            __import__(package.replace('-', '_'))
        except ImportError:
            missing_packages.append(package)
    
    if missing_packages:
        print("âŒ Packages manquants pour la plateforme tokenizer:")
        for pkg in missing_packages:
            print(f"   - {pkg}")
        print(f"\nğŸ“¦ Installez-les avec:")
        print(f"pip install {' '.join(missing_packages)}")
        return False
    
    print("âœ… DÃ©pendances tokenizer installÃ©es")
    return True

def download_nltk_data():
    """TÃ©lÃ©charge les donnÃ©es NLTK nÃ©cessaires"""
    try:
        import nltk
        print("ğŸ“¥ TÃ©lÃ©chargement des donnÃ©es NLTK...")
        
        nltk_downloads = [
            'stopwords',
            'punkt', 
            'averaged_perceptron_tagger',
            'wordnet'
        ]
        
        for item in nltk_downloads:
            try:
                nltk.download(item, quiet=True)
            except Exception as e:
                print(f"âš ï¸  Impossible de tÃ©lÃ©charger {item}: {e}")
        
        print("âœ… DonnÃ©es NLTK prÃªtes")
        return True
        
    except Exception as e:
        print(f"âŒ Erreur tÃ©lÃ©chargement NLTK: {e}")
        return False

def setup_spacy_models():
    """Configure les modÃ¨les spaCy de base"""
    try:
        print("ğŸ“¥ VÃ©rification des modÃ¨les spaCy...")
        
        # Essayer de charger un modÃ¨le multilingue de base
        try:
            import spacy
            from spacy.lang.xx import MultiLanguage
            nlp = MultiLanguage()
            print("âœ… ModÃ¨le spaCy multilingue disponible")
        except Exception:
            print("âš ï¸  ModÃ¨le spaCy multilingue non disponible (optionnel)")
        
        return True
        
    except Exception as e:
        print(f"âš ï¸  Configuration spaCy: {e}")
        return True  # Non bloquant

def create_tokenizer_directories():
    """CrÃ©e les dossiers nÃ©cessaires pour le tokenizer"""
    directories = [
        'custom_tokenizers',
        'multilingual_corpus',
        'trained_tokenizer_models',
        'tokenizer_analysis'
    ]
    
    for directory in directories:
        Path(directory).mkdir(exist_ok=True)
        print(f"ğŸ“ Dossier tokenizer crÃ©Ã©: {directory}")

def start_tokenizer_api():
    """DÃ©marre l'API tokenizer"""
    print("ğŸš€ DÃ©marrage de l'API Universal Tokenizer...")
    
    try:
        api_process = subprocess.Popen([
            sys.executable, "-m", "uvicorn",
            "universal_tokenizer_api:app",
            "--host", "0.0.0.0", 
            "--port", "8008",
            "--reload"
        ])
        
        print("âœ… API Tokenizer dÃ©marrÃ©e sur http://localhost:8008")
        return api_process
        
    except Exception as e:
        print(f"âŒ Erreur dÃ©marrage API Tokenizer: {e}")
        return None

def start_tokenizer_dashboard():
    """DÃ©marre l'interface Streamlit du tokenizer"""
    print("ğŸ¨ DÃ©marrage de l'interface Tokenizer...")
    
    # Attendre que l'API soit prÃªte
    print("â³ Attente du dÃ©marrage de l'API Tokenizer...")
    time.sleep(8)
    
    try:
        streamlit_process = subprocess.Popen([
            sys.executable, "-m", "streamlit", "run",
            "universal_tokenizer_dashboard.py",
            "--server.port", "8009",
            "--server.address", "localhost"
        ])
        
        print("âœ… Interface Tokenizer dÃ©marrÃ©e sur http://localhost:8009")
        return streamlit_process
        
    except Exception as e:
        print(f"âŒ Erreur dÃ©marrage interface Tokenizer: {e}")
        return None

def check_integration_with_ai_platform():
    """VÃ©rifie la disponibilitÃ© de la plateforme IA pour intÃ©gration"""
    try:
        import requests
        response = requests.get("http://localhost:8006/health", timeout=2)
        if response.status_code == 200:
            print("ğŸ”— AI Training Platform dÃ©tectÃ©e - intÃ©gration disponible")
            return True
    except:
        pass
    
    print("â„¹ï¸  AI Training Platform non disponible - fonctionnera en mode standalone")
    return False

def main():
    """Fonction principale de dÃ©marrage"""
    print("ğŸ”¤ DÃ©marrage de la Universal Tokenizer Platform")
    print("=" * 55)
    
    # VÃ©rifications prÃ©liminaires
    if not check_tokenizer_dependencies():
        print("\nâŒ Installez les dÃ©pendances manquantes avant de continuer")
        return
    
    # Configuration initiale
    download_nltk_data()
    setup_spacy_models()
    create_tokenizer_directories()
    
    # VÃ©rifier l'intÃ©gration avec la plateforme IA
    ai_integration = check_integration_with_ai_platform()
    
    # DÃ©marrer les services
    api_process = start_tokenizer_api()
    if not api_process:
        print("âŒ Impossible de dÃ©marrer l'API Tokenizer")
        return
    
    dashboard_process = start_tokenizer_dashboard()
    if not dashboard_process:
        print("âŒ Impossible de dÃ©marrer l'interface Tokenizer")
        api_process.terminate()
        return
    
    print("\nğŸ‰ Universal Tokenizer Platform dÃ©marrÃ©e avec succÃ¨s!")
    print("ğŸ”¤ API Tokenizer: http://localhost:8008")
    print("ğŸ¨ Interface Tokenizer: http://localhost:8009")
    print("ğŸ“š Documentation API: http://localhost:8008/docs")
    
    if ai_integration:
        print("ğŸ¤– IntÃ©gration IA disponible: http://localhost:8006")
        print("ğŸ”— Dashboard IA: http://localhost:8007")
    
    print("\nğŸ“‹ FonctionnalitÃ©s disponibles:")
    print("   â€¢ Tokenisation multilingue universelle")
    print("   â€¢ Analyse linguistique avancÃ©e")
    print("   â€¢ EntraÃ®nement de tokenizers personnalisÃ©s")
    print("   â€¢ Gestion de corpus multilingues")
    print("   â€¢ Comparaison de performances")
    print("   â€¢ Analytics et statistiques")
    if ai_integration:
        print("   â€¢ IntÃ©gration avec AI Training Platform")
    
    print("\nâš ï¸  Appuyez sur Ctrl+C pour arrÃªter tous les services")
    
    try:
        # Boucle de monitoring
        while True:
            time.sleep(5)
            
            # VÃ©rifier que les processus sont toujours vivants
            if api_process.poll() is not None:
                print("âš ï¸  API Tokenizer s'est arrÃªtÃ©e de maniÃ¨re inattendue")
                break
                
            if dashboard_process.poll() is not None:
                print("âš ï¸  Interface Tokenizer s'est arrÃªtÃ©e de maniÃ¨re inattendue")
                break
                
    except KeyboardInterrupt:
        print("\nğŸ›‘ ArrÃªt des services tokenizer...")
        
        # ArrÃªter les processus
        if dashboard_process:
            dashboard_process.terminate()
            print("âœ… Interface Tokenizer arrÃªtÃ©e")
            
        if api_process:
            api_process.terminate() 
            print("âœ… API Tokenizer arrÃªtÃ©e")
        
        print("âœ… Universal Tokenizer Platform arrÃªtÃ©e avec succÃ¨s")

if __name__ == "__main__":
    main()